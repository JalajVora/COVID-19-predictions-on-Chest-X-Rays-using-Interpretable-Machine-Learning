---
title: "Untitled"
author: "Name"
date: "30/05/2020"
output: 
  html_document: 
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE,echo = T,tidy =  FALSE,highlight = TRUE,
                      error=F, fig.width=12, fig.height = 4)
options(scipen = 999)
```

```{r, echo=F}
load('data.RData')
#load('C:\Users\admin1\Desktop\New folder\data')
```




## Image Pre-Processing

```{r}
#----------------libraries used
library(tidyverse)          # data manipulation
library(imager)
library(repr)
library(rpart) #classification and regression trees
library(partykit) #treeplots
library(e1071) # SVM
library(randomForest)
library(C50)
library(factoextra)
library(caret) #tune hyper-parameters
library(pander)
library(klaR)
```


Next, we use the first image from the covid-positive X-ray results dataset as a test for demonstration purposes.

```{r}
# read images 
directoryPos = paste(getwd(),"/","covid-positive", sep = "")
directoryNeg = paste(getwd(),"/","covid-negative", sep = "")
```

```{r, eval=F}
filesCovidPos = list.files(path=directoryPos,pattern = "*.jpg", full.names=TRUE) 
filesCovidNeg = list.files(path=directoryNeg,pattern = "*.jpeg", full.names=TRUE) 
imagesCovidPos <- lapply(filesCovidPos, load.image )
imagesCovidNeg <- lapply(filesCovidNeg, load.image )
```


```{r, eval=F}
test_image <- load.image(filesCovidPos[2])

# resizing, 
resize_height <- 28
resize_width <- 28
resized_test_image <- resize(test_image, resize_width, resize_height)
pixels_to_crop <- 0
cropped_test_image <- crop.borders(resized_test_image, nPix = pixels_to_crop)
new_height <- dim(cropped_test_image)[1]
new_width <- dim(cropped_test_image)[2]

```

```{r}
# picture of the X-rays
plot(cropped_test_image)
```


Next, we inititalize our dataframe for every picture of covid-positive X-ray results for the training set:

```{r, eval=F}
df_COVID19 <- data.frame(matrix(NA, nrow = length(filesCovidPos), ncol = new_height * new_width)) 
```

The most important (and lengthy) part of the process of data wrangling is going to be fitting every picture within the working directory into the dataframe we have just created.

```{r, eval=F}
# very slow, 5 pictures per second (ie 5 minutes loading a 1341 picture dataset, but what gives, right?)
for (i in 1:length(filesCovidPos)) {
    im <- load.image(filesCovidPos[i])
    thmb <- resize(im, resize_height, resize_width) %>% crop.borders(nPix = pixels_to_crop) #%>% imsub(x %inr% c(1,3))
    df_COVID19 [i,] <- t(as.vector(thmb))
}
df_COVID19$y <- 1
```

To get finished with the class of covid-positive results, we add the corresponding label as a new column of our dataframe.

```{r}

str(df_COVID19)
```

The following repeats the dataset building process for labels of the class Negative.

```{r, eval=F}
test_image <- load.image(filesCovidNeg[2])
resized_test_image <- resize(test_image, resize_width, resize_height)
cropped_test_image <- crop.borders(resized_test_image, nPix = pixels_to_crop)
plot(cropped_test_image)

df_Negative <- data.frame(matrix(NA, nrow = length(filesCovidNeg), ncol = new_height * new_width))

start.time <- Sys.time()
for (i in 1:length(filesCovidNeg)) {
    im <- load.image(filesCovidNeg[i])
    thmb <- resize(im, resize_height, resize_width) %>% crop.borders(nPix = pixels_to_crop) #%>% imsub(x %inr% c(1,3))
    df_Negative[i,] <- t(as.vector(thmb))
}

end.time <- Sys.time()
interval <- end.time - start.time
print(interval)


df_Negative$y <- 0
```

```{r}
str(df_Negative)
```


The results of our steps so far are two dataframes, one with X_Negative + Y_Negative, the other with X_COVID19 + Y_COVID19. In the following script we bind the two dataframes, and then randomly select observations for our training and testing sets.

```{r}
dataset <- rbind(df_COVID19, df_Negative) %>%
    mutate(y = factor(y, labels   = c('Normal','COVID19')))

dataset$y <- relevel(dataset$y, ref="COVID19")

# Create Training and Test data 
set.seed(1234)
training_data <- dataset %>%
    dplyr::select(y) %>%
    unlist() %>%
    createDataPartition(p = 0.75, list = FALSE)
X_train <- dataset %>%
    dplyr::select(-y) %>%
    slice(training_data) %>%
    data.frame()
Y_train <- dataset %>%
    dplyr::select(y) %>%
    slice(training_data) %>%
    unlist()
X_test <- dataset %>%
    dplyr::select(-y) %>%
    slice(-training_data) %>%
    data.frame()
Y_test <- dataset %>%
    dplyr::select(y) %>%
    slice(-training_data) %>%
    unlist()

```



## Statistics on data sets, the number of negative /positive samples

```{r}
ft_orig <- table(dataset$y)
ft_train <- table(Y_train)
ft_test <- table(Y_test)

ft_df <- as.data.frame(cbind(ft_orig, ft_train, ft_test))
colnames(ft_df) <- c("Original", "Training set", "Test set")
pander::pander(ft_df, style="rmarkdown",
       caption=paste0("Negative /Positive frequencies among datasets"))
```


```{r}
dataset %>% janitor::tabyl(y) %>% 
  mutate(percent=scales::percent(percent)) %>% 
ggplot(aes(x=y, fill=y,y=n))+
  geom_bar(stat = 'identity')+
  ylim(c(0,400))+
  theme_bw()+
  geom_text(aes(x=y, y=300, label=percent))+
  theme(legend.position = 'none')

```

## Feature extraction

```{r}
# calculate correlation matrix
corrMat <- cor(X_train)
dim(corrMat)

# find attributes that are highly corrected (ideally >0.75)
set.seed(1234)
highCorr <-findCorrelation(corrMat, cutoff = .75)# returned that indicates which columns should be removed.
length(highCorr)
head(highCorr)
filteredX_train <- X_train[, -highCorr]
dim(filteredX_train)
```




```{r, eval=F}
## Compute the area under the ROC curve, sensitivity, specificity,
## accuracy and Kappa

fiveStats <- function(...) c(twoClassSummary(...),
                              defaultSummary(...))
 ## Create resampling data sets to use for all models
set.seed(104)
index <- createMultiFolds(Y_train, times = 5)

## The built-in random forest functions are in rfFuncs.
str(rfFuncs)
newRF <- rfFuncs
newRF$summary <- fiveStats
subsets <- c(1:5, 10, 15, 20, 25)
# define the control using a random forest selection function
ctrl <- rfeControl(method = "cv", number=10,
                   functions = newRF,
                   verbose = T)
# run the RFE algorithm
set.seed(721)
rfRFE <- rfe(x = filteredX_train,
             y = Y_train,
             metric = "ROC",
             rfeControl = ctrl,
             sizes= subsets,
              tuneLength = 12,
             ## now pass options to randomForest()
             ntree = 1000)


# list the chosen features

ReducedSet <- rfRFE$variables %>% arrange(desc(COVID19)) %>% filter(!duplicated(var)) %>% head(100)

```

```{r}
# summarize the results
print(rfRFE)
# plot the results
trellis.par.set(caretTheme())
plot(rfRFE, type = c("g", "o"))
```





The selection algorithm indicates that the subset we use for the clustering model is composed of variables `r top100$var` and that other variables should be rejected.

```{r}
# The top 5 variables (out of 225):
 #  X757, X760, X176, X131, X189

filteredX_train <- filteredX_train %>%
  dplyr::select(ReducedSet$var)
dim(filteredX_train)

TestData <- X_test %>% dplyr::select( names(filteredX_train))
```


#### Naive Bayes 

We will now use Naive Bayes to train a couple of prediction models. Both models will be generated using 10-fold cross validation, with the default parameters.

The difference between the models will be that the first one does not use the Laplace correction and lets the training procedure figure out whether to user or not a kernel density estimate, while the second one searches Laplace parameter from 0 to 5 ( best fL=0) and explicitly forbids the use of a kernel density estimate (useKernel=FALSE).

```{r,eval=F}

#Naive algorithm
set.seed(1234)
NB_Model1 <- train(filteredX_train, Y_train, 
                    method = "nb",
                    metric = "ROC",
                    #preProcess=c("scale","center"),
                    trControl= train_control)
```


```{r}
# results
confusionMatrix(NB_Model1)

```

```{r, eval=F}
set.seed(1234)
# set up tuning grid
search_grid <- expand.grid(
  usekernel =  FALSE,
  fL = 0:5,
  adjust = seq(0, 5, by = 1))
NB_Model2 <- train(filteredX_train, Y_train, 
                    method = "nb",
                    metric = "ROC",
                   tuneGrid=search_grid,
                    #preProcess=c("scale","center"),
                    trControl= train_control)
``` 

```{r}
# plot search grid results
plot(NB_Model2)

# results for best model
confusionMatrix(NB_Model2)
```



We can assess the accuracy on our final holdout test set.



```{r}
# Visualize Naive Bayes clustering
predY <- predict(NB_Model2$finalModel, filteredX_train, na.action = na.pass)

fviz_cluster(list(data=filteredX_train, cluster=predY$class),
             ellipse.type = 'norm',geom = 'point',
             stand = F, palette='jco', 
             main='Cluster Analysis for COVID19 Using Naive Bayes  classifier',
             ggtheme = theme_classic())
```

```{r}
#Predictions
Ypred_NB1 <- predict(NB_Model1, TestData)
Ypred_NB2 <- predict(NB_Model2$finalModel, TestData)
tab_NB1 <- confusionMatrix( data= Ypred_NB1,  reference= Y_test)
tab_NB2 <- confusionMatrix( data= Ypred_NB2$class,  reference=Y_test)
NB1 <- ConfMat(tab_NB1)
NB2 <- ConfMat(tab_NB2)
```


```{r}

model_comp <- as.data.frame(rbind(NB1,NB2))
rownames(model_comp) <- c("naive Bayes Basic", "naive Bayes _ Laplace")
pander::pander(model_comp, style="rmarkdown", split.tables=Inf, keep.trailing.zeros=TRUE,
       caption="Model results when comparing test set")
```

Looking at accuracy alone, the model seems quite robust. However taking into account sensitivity, we see that our model is yet to be further improved.



